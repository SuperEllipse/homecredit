{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789d812b-26d9-495e-b9d7-5217856deecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "db_path=\"s3a://tsel-bucket/data/warehouse/tablespace/managed/hive/\"\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"homecredit-spark\")\n",
    "    .config(\"spark.sql.warehouse.dir\", db_path)\n",
    "    .config(\"spark.hadoop.fs.s2a.s3guard.ddb.region\", \"us-east-1\")\n",
    "#    .config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystem\",\"s3a://tsel-bucket/\")\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    .master(\"local[5]\") # should be possible to change this to SPARK on Yarn or SPARK on Kubernetes\n",
    "    .getOrCreate())\n",
    "\n",
    "homecredit_raw_df=spark.read.option(\"header\", True) \\\n",
    "                    .option(\"inferSchema\",True) \\\n",
    "                    .csv(\"/home/cdsw/04_RAW_TRAINING_DATA/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1b94c9-4496-4d81-8f27-5c4efcf01bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356251, 722)\n"
     ]
    }
   ],
   "source": [
    "print((homecredit_raw_df.count(), len(homecredit_raw_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a4a0c3-a50f-4e86-8356-4372083808a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exprs = [col(column).alias(column.replace(' ', '_')) for column in homecredit_df.columns]\n",
    "# homecredit_formatted_df = homecredit_df.select(*exprs)\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "def normalize(column: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize column name by replacing invalid characters with underscore\n",
    "    strips accents and make lowercase\n",
    "    :param column: column name\n",
    "    :return: normalized column name\n",
    "    \"\"\"\n",
    "    n = re.sub(r\"[ ,;:{}()/\\n\\t=\\-\\+]+\", '_', column.lower())\n",
    "    return unicodedata.normalize('NFKD', n).encode('ASCII', 'ignore').decode()\n",
    "\n",
    "\n",
    "# using the function\n",
    "homecredit_df = homecredit_raw_df.toDF(*map(normalize, homecredit_raw_df.columns))\n",
    "homecredit_df=homecredit_df.drop(col(\"_c0\"))\n",
    "\n",
    "\n",
    "homecredit_df = homecredit_df.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\n",
    "window = Window.orderBy(col('monotonically_increasing_id'))\n",
    "homecredit_df = homecredit_df.withColumn('increasing_id', row_number().over(window))\n",
    "\n",
    "## cast to float\n",
    "cast_cols =[\"new_phone_to_birth_ratio_employer\", \"prev_app_credit_perc_max\", \"refused_app_credit_perc_max\", \"instal_payment_perc_max\", \"instal_payment_perc_min\" ]\n",
    "for col_name in cast_cols:\n",
    "    homecredit_df = homecredit_df.withColumn(col_name, col(col_name).cast('double'))\n",
    "\n",
    "\n",
    "homecredit_df = homecredit_df.withColumn(\"created\", current_timestamp())\n",
    "homecredit_df = homecredit_df.withColumn(\"event_timestamp\", current_timestamp() - expr(\"INTERVAL 1 seconds\") * col(\"increasing_id\"))\n",
    "homecredit_df =homecredit_df.drop(col(\"monotonically_increasing_id\"))\n",
    "homecredit_df = homecredit_df.drop(col(\"increasing_id\")) \n",
    "\n",
    "#homecredit_df=homecredit_df.na.drop() -- we just get 58 rows if we use drop nas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb34284-6dac-4984-a54b-11b1f3fbb2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356251, 723)\n"
     ]
    }
   ],
   "source": [
    "print((homecredit_df.count(), len(homecredit_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75c6b327-c2b2-4f37-a05f-d70ad87cf12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = c5a12e6b-9edf-4b81-b2a9-bd83177e68b8\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the processed formatted files to hive\n",
    "\n",
    "# homecreditsample_df = homecredit_df.select(col( \"event_timestamp\" ), col(\"sk_id_curr\"), col(\"flag_own_car\"), col(\"flag_own_realty\"),col(\"cnt_children\"), col(\"created\")).limit(20)\n",
    "# homecreditsample_df = homecredit_df.limit(20)\n",
    "                                           \n",
    "homecredit_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .saveAsTable('homecredit_processed_data')\n",
    "\n",
    "# homecreditsample_df.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .saveAsTable('homecredit_processed_data_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94333b17-2a9b-48ed-a95f-bf46ad8a4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the Feature Definition File FEAST Datatype Mappings\n",
    "def spark_type_to_primitive_feast_value_type(\n",
    "    name: str\n",
    ") -> str:\n",
    "    \n",
    "    \n",
    "    type_map = {\n",
    "        \"int\": \"Int32\",\n",
    "        \"bigint\": \"Int64\",\n",
    "        \"float\": \"Float32\",\n",
    "        \"string\": \"String\",\n",
    "        \"double\": \"Float32\",\n",
    "        \"timestamp\": \"TIMESTAMP\",\n",
    "\n",
    "    }\n",
    "    return type_map[name]\n",
    "\n",
    "## Copy the output generated here into the feature specifications file in Feature Repo. This is needed because FEAST doesn't seem to infer features properly by itself. \n",
    "\n",
    "for i in homecredit_df.dtypes:\n",
    "    name =i[0]\n",
    "    pDtype= i[1]\n",
    "    if name != \"sk_id_curr\" and name != \"event_timestamp\" and name != \"created\" :\n",
    "        print(f\"Field(name=\\\"{name}\\\" , dtype={spark_type_to_primitive_feast_value_type(pDtype)}),\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e14332-f797-41a9-855d-fbff5c55b638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88560b9-9fea-4786-b1fc-94fb186300d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark app setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "db_path=\"s3a://se-indonesia-cdp/data/warehouse/tablespace/managed/hive\"\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"homecredit-spark\")\n",
    "    .config(\"spark.sql.warehouse.dir\", db_path)\n",
    "    .config(\"spark.hadoop.fs.s2a.s3guard.ddb.region\", \"us-east-1\")\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://se-indonesia-cdp/\")\n",
    "    .master(\"local[5]\") # should be possible to change this to SPARK on Yarn or SPARK on Kubernetes\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c224f-8833-49a7-af2a-e2c263f83cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE CODE \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "# Method 1: \n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ('Alice','10'),('Susan','12')\n",
    "    ],\n",
    "    ['Name','Age']\n",
    ")\n",
    "\n",
    "\n",
    "df1=df.rdd.zipWithIndex().toDF()\n",
    "df2=df1.select(col(\"_1.*\"),col(\"_2\").alias('increasing_id'))\n",
    "df2.show()\n",
    "\n",
    "# -- Method2 \n",
    "df_with_increasing_id = df.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\n",
    "df_with_increasing_id.show()\n",
    "\n",
    "# Method3\n",
    "window = Window.orderBy(col('monotonically_increasing_id'))\n",
    "df_with_consecutive_increasing_id = df_with_increasing_id.withColumn('increasing_id', row_number().over(window))\n",
    "df_with_consecutive_increasing_id.show()\n",
    "\n",
    "# Method3a - Just using row_id\n",
    "df3 =df_with_consecutive_increasing_id.drop(col(\"monotonically_increasing_id\"))\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ed840e-1199-44a4-91ce-ff6e8e0f5a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------------------+----------------------+\n",
      "|id |current_date|current_timestamp     |decreased_timestamp   |\n",
      "+---+------------+----------------------+----------------------+\n",
      "|1  |2022-12-01  |2022-12-01 07:18:50.14|2022-12-01 07:16:50.14|\n",
      "|2  |2022-12-01  |2022-12-01 07:18:50.14|2022-12-01 07:14:50.14|\n",
      "|3  |2022-12-01  |2022-12-01 07:18:50.14|2022-12-01 07:12:50.14|\n",
      "+---+------------+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()\n",
    "data=[[\"1\"], [\"2\"], [\"3\"]]\n",
    "df=spark.createDataFrame(data,[\"id\"])\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#current_date() & current_timestamp()\n",
    "df.withColumn(\"current_date\",current_date()) \\\n",
    "  .withColumn(\"current_timestamp\",current_timestamp()) \\\n",
    "  .withColumn(\"decreased_timestamp\", current_timestamp() - expr(\"INTERVAL 2 minutes\") * col(\"id\"))  \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec9f42-e1b9-453f-9759-6d7183e6653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.select(col(\"index\"), col(\"sk_id_curr\"), col(\"increasing_id\"), col(\"target\"), col(\"code_gender\"), col(\"amt_income_total\")).show()\n",
    "df3.createOrReplaceTempView(\"homecredit\")\n",
    "#query_string = '''select index, sk_id_curr, target, code_gender, amt_income_total, cast(current_timestamp as TIMESTAMP) - (INTERVAL 1 minutes) * increasing_id as event_timestamp, from homecredit'''\n",
    "query_string = '''select index, sk_id_curr, target, code_gender, amt_income_total, cast(current_timestamp as TIMESTAMP) - (INTERVAL 1 minutes) * index as event_timestamp from homecredit'''\n",
    "\n",
    "spark.sql( query_string).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

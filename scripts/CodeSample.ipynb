{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88560b9-9fea-4786-b1fc-94fb186300d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark app setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "db_path=\"s3a://se-indonesia-cdp/data/warehouse/tablespace/managed/hive\"\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"homecredit-spark\")\n",
    "    .config(\"spark.sql.warehouse.dir\", db_path)\n",
    "    .config(\"spark.hadoop.fs.s2a.s3guard.ddb.region\", \"us-east-1\")\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://se-indonesia-cdp/\")\n",
    "    .master(\"local[5]\") # should be possible to change this to SPARK on Yarn or SPARK on Kubernetes\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1c224f-8833-49a7-af2a-e2c263f83cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n",
      "| Name|Age|increasing_id|\n",
      "+-----+---+-------------+\n",
      "|Alice| 10|            0|\n",
      "|Susan| 12|            1|\n",
      "+-----+---+-------------+\n",
      "\n",
      "+-----+---+---------------------------+\n",
      "| Name|Age|monotonically_increasing_id|\n",
      "+-----+---+---------------------------+\n",
      "|Alice| 10|                17179869184|\n",
      "|Susan| 12|                34359738368|\n",
      "+-----+---+---------------------------+\n",
      "\n",
      "+-----+---+---------------------------+-------------+\n",
      "| Name|Age|monotonically_increasing_id|increasing_id|\n",
      "+-----+---+---------------------------+-------------+\n",
      "|Alice| 10|                17179869184|            1|\n",
      "|Susan| 12|                34359738368|            2|\n",
      "+-----+---+---------------------------+-------------+\n",
      "\n",
      "+-----+---+-------------+\n",
      "| Name|Age|increasing_id|\n",
      "+-----+---+-------------+\n",
      "|Alice| 10|            1|\n",
      "|Susan| 12|            2|\n",
      "+-----+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE CODE \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "# Method 1: \n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ('Alice','10'),('Susan','12')\n",
    "    ],\n",
    "    ['Name','Age']\n",
    ")\n",
    "\n",
    "\n",
    "df1=df.rdd.zipWithIndex().toDF()\n",
    "df2=df1.select(col(\"_1.*\"),col(\"_2\").alias('increasing_id'))\n",
    "df2.show()\n",
    "\n",
    "# -- Method2 \n",
    "df_with_increasing_id = df.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\n",
    "df_with_increasing_id.show()\n",
    "\n",
    "# Method3\n",
    "window = Window.orderBy(col('monotonically_increasing_id'))\n",
    "df_with_consecutive_increasing_id = df_with_increasing_id.withColumn('increasing_id', row_number().over(window))\n",
    "df_with_consecutive_increasing_id.show()\n",
    "\n",
    "# Method3a - Just using row_id\n",
    "df3 =df_with_consecutive_increasing_id.drop(col(\"monotonically_increasing_id\"))\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d57aae-efed-4585-97a0-f7da05ef97db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------------------+-----------------------+\n",
      "|id |current_date|current_timestamp      |event_timestamp        |\n",
      "+---+------------+-----------------------+-----------------------+\n",
      "|1  |2022-12-07  |2022-12-07 05:57:56.482|2022-10-07 05:57:46.482|\n",
      "|2  |2022-12-07  |2022-12-07 05:57:56.482|2022-10-07 05:57:36.482|\n",
      "|3  |2022-12-07  |2022-12-07 05:57:56.482|2022-10-07 05:57:26.482|\n",
      "|4  |2022-12-07  |2022-12-07 05:57:56.482|2022-10-07 05:57:16.482|\n",
      "|5  |2022-12-07  |2022-12-07 05:57:56.482|2022-10-07 05:57:06.482|\n",
      "|1  |2022-12-07  |2022-12-07 05:57:56.482|2024-12-07 05:58:06.482|\n",
      "|2  |2022-12-07  |2022-12-07 05:57:56.482|2024-12-07 05:58:16.482|\n",
      "|3  |2022-12-07  |2022-12-07 05:57:56.482|2024-12-07 05:58:26.482|\n",
      "|4  |2022-12-07  |2022-12-07 05:57:56.482|2024-12-07 05:58:36.482|\n",
      "|5  |2022-12-07  |2022-12-07 05:57:56.482|2024-12-07 05:58:46.482|\n",
      "+---+------------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# Spark app setup\n",
    "from pyspark.sql import SparkSession\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()\n",
    "data=[[\"1\"], [\"2\"], [\"3\"], [\"4\"], [\"5\"]]\n",
    "df=spark.createDataFrame(data,[\"id\"])\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#current_date() & current_timestamp()\n",
    "df=df.withColumn(\"current_date\",current_date()) \\\n",
    "  .withColumn(\"current_timestamp\",current_timestamp()) \\\n",
    "  .withColumn(\"event_timestamp\", current_timestamp()- expr(\"INTERVAL 2 Month\")  - expr(\"INTERVAL 10 seconds\") * col(\"id\"))  \n",
    "schema = df.schema\n",
    "df_pd = df.toPandas()\n",
    "df1 = spark.createDataFrame(df_pd, schema=schema)\n",
    "del df_pd\n",
    "df1 = df1.withColumn(\"current_date\",current_date()) \\\n",
    "  .withColumn(\"current_timestamp\",current_timestamp()) \\\n",
    "  .withColumn(\"event_timestamp\", current_timestamp()+ expr(\"INTERVAL 2 Years\")  + expr(\"INTERVAL 10 seconds\") * col(\"id\"))  \\\n",
    "\n",
    "df.union(df1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec9f42-e1b9-453f-9759-6d7183e6653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.select(col(\"index\"), col(\"sk_id_curr\"), col(\"increasing_id\"), col(\"target\"), col(\"code_gender\"), col(\"amt_income_total\")).show()\n",
    "df3.createOrReplaceTempView(\"homecredit\")\n",
    "#query_string = '''select index, sk_id_curr, target, code_gender, amt_income_total, cast(current_timestamp as TIMESTAMP) - (INTERVAL 1 minutes) * increasing_id as event_timestamp, from homecredit'''\n",
    "query_string = '''select index, sk_id_curr, target, code_gender, amt_income_total, cast(current_timestamp as TIMESTAMP) - (INTERVAL 1 minutes) * index as event_timestamp from homecredit'''\n",
    "\n",
    "spark.sql( query_string).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96a067-6d6f-4832-9c1a-b871993512ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -r /home/cdsw/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e87d89c1-5e87-4961-bffa-56d9fe5080c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = { \n",
    "    \"train\": {\n",
    "        \"path\":\"/home/cdsw/02_RAW_TRAINING_DATA/processed_data.csv\",\n",
    "         \"store_name\": \"homecredit_processed_data_train\"\n",
    "    },\n",
    "    \n",
    "    \"test\": {\n",
    "        \"path\":\"/home/cdsw/02_PREDICTION_DATA/processed_data.csv\",\n",
    "         \"store_name\": \"homecredit_processed_data_test\"\n",
    "    },\n",
    "    \n",
    "            \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2999ec04-077f-4eb8-acfc-9b67cb71d3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cdsw/02_RAW_TRAINING_DATA/processed_data.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict[\"train\"][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd83713-7fd2-4418-9f46-5f9ca4d43b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'path': '/home/cdsw/04_RAW_TRAINING_DATA/processed_data.csv', 'sourcedatatype': 'train'}, 'test': {'path': '/home/cdsw/02_PREDICTION_DATA/processed_data.csv', 'sourcedatatype': 'test'}}\n"
     ]
    }
   ],
   "source": [
    "#dynamic dictionary\n",
    "\n",
    "import os \n",
    "datasource_dict = { \n",
    "    \"train\": {\n",
    "        \"path\":\"/home/cdsw/04_RAW_TRAINING_DATA/processed_data.csv\",\n",
    "         \"sourcedatatype\": \"train\"\n",
    "    },\n",
    "    \n",
    "    \"test\": {\n",
    "        \"path\":\"/home/cdsw/02_PREDICTION_DATA/processed_data.csv\",\n",
    "         \"sourcedatatype\": \"test\"\n",
    "    },\n",
    "    \n",
    "            \n",
    "}\n",
    "\n",
    "datasource_dict1 = { \"train\" : {}, \n",
    "                    \"test\": {}\n",
    "                  }\n",
    "datasource_dict1[\"train\"][\"path\"]  = os.environ[\"DATASOURCE_PATH_TRAIN\"]\n",
    "datasource_dict1[\"test\"][\"path\"] = os.environ[\"DATASOURCE_PATH_TEST\"]\n",
    "\n",
    "datasource_dict1[\"train\"][\"sourcedatatype\"] = \"train\"\n",
    "datasource_dict1[\"test\"][\"sourcedatatype\"] = \"test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b7f94-4701-4f7f-a7b3-f84daedce854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
